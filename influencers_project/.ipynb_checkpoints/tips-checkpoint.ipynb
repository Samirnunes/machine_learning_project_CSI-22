{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe35f25d",
   "metadata": {},
   "source": [
    "## Important imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d7131ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# sklearn is essential, but it has many different imports, so I won't put it here.\n",
    "from import_data import import_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a154eed4",
   "metadata": {},
   "source": [
    "## How to get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c2dda63",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, y_train, X_train, X_test = import_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e942cd",
   "metadata": {},
   "source": [
    "## Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce91e9b",
   "metadata": {},
   "source": [
    "### Data analysis\n",
    "\n",
    "__-Univariate__:\n",
    "\n",
    "$\\rightarrow$ Analyses each feature in X_train and X_test dataframes, and the labels in the y_train dataframe individually. \n",
    "\n",
    "$\\rightarrow$ Histograms, Boxplots, Bar graphics and Swarmplots are a good beginning.\n",
    "\n",
    "$\\rightarrow$ df.describe() to see metrics, df.shape to see number of rows and columns, df.dtypes to see features types, df.info() to see many informations, such as the number of NaN's in the data.\n",
    "\n",
    "__-Multivariate__:\n",
    "\n",
    "$\\rightarrow$ Analyses the relation between features or between features and labels.\n",
    "\n",
    "$\\rightarrow$ Common approaches are plotting Scatterplots, Heatmap of correlations, Lineplots (if a variable evolves through another one), Bar graphics, Density plots, Hexagonal Compartment plots and Linear Regression plots.\n",
    "\n",
    "$\\rightarrow$ df.corr(numeric_only = True) to get the correlations between all numerical features.\n",
    "\n",
    "$\\rightarrow$ It's very important to analyse not only the relation between features and label! The relation between features is also essential to determine if we have repetitive or ambiguous information, which can cause problems in the model's prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300acf09",
   "metadata": {},
   "source": [
    "### Preprocessing:\n",
    "\n",
    "__- Missing values detection and treatment:__\n",
    "\n",
    "$\\rightarrow$ Detection: X_train.info(), X_test.info() or pd.DataFrame.isna().sum() (for number of NaN's in each column) or pd.DataFrame.isna().sum().sum() (for total number of NaN's).\n",
    "\n",
    "$\\rightarrow$ Treatment: Imputation (mode, mean, median), drop or, if just one class is missing, NaN may be substituted by a value (e.g. 0).\n",
    "\n",
    "__- Outliers detection and treatment:__\n",
    "\n",
    "$\\rightarrow$ Detection: through metrics (standard deviation, for example), histograms, boxplots (any point after Q3 + 1.5 * interquartile_distance or before Q1 - 1.5 * interquartile_distance is classified as outlier, where Q3 is the 3rd quartile and Q1 the 1st quartile), or through Z-score (any point that falls out of 3rd standard deviation is classified as outlier).\n",
    "\n",
    "$\\rightarrow$ Treatment: log scaling and clipping are common solutions.\n",
    "\n",
    "__-Imbalanced classes detection and treatment:__\n",
    "\n",
    "$\\rightarrow$ Detection: X_train.value_counts(), X_test.value_counts(). If the number of elements of a class is much higher than the other ones, then we say that the dataset has imbalanced data.\n",
    "\n",
    "$\\rightarrow$ Treatment: possible solution is downsampling and upwweighting. Explanation in the link: https://developers.google.com/machine-learning/data-prep/construct/sampling-splitting/imbalanced-data.\n",
    "\n",
    "__-Categorical data to numerical detection and treatment:__\n",
    "\n",
    "$\\rightarrow$ Detection: X_train.dtypes, X_test.dtypes, y_train.dtypes.\n",
    "\n",
    "$\\rightarrow$ Treatment: One-Hot Encoding (if classes don't have a specific order) or Ordinal Encoding (if classes have an order) are common solutions.\n",
    "\n",
    "__-Creating new features__:\n",
    "\n",
    "$\\rightarrow$ This is almost an art. We need to think if a combination (maybe a sum, or a product) of features will be a better predictor, for example. Or we can separate a feature into bins to help our model to learn specific characteristics of these bins. We can also extract informations from categorical features without making any numerical transformation (remember Titanic's title extraction from the 'Name' feature and deck from the 'Cabin'), or maybe we can limit a feature information (remember Titanic's feature 'Traveled_Alone', which has been extracted from 'Family_Size' feature). \n",
    "\n",
    "$\\rightarrow$ The summary: it is practically an art!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
